Return-Path: <linux-kernel-owner@vger.kernel.org>
X-Original-To: lists+linux-kernel@lfdr.de
Delivered-To: lists+linux-kernel@lfdr.de
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.lfdr.de (Postfix) with ESMTP id E5BBB14789
	for <lists+linux-kernel@lfdr.de>; Mon,  6 May 2019 11:21:10 +0200 (CEST)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726283AbfEFJVC (ORCPT <rfc822;lists+linux-kernel@lfdr.de>);
        Mon, 6 May 2019 05:21:02 -0400
Received: from usa-sjc-mx-foss1.foss.arm.com ([217.140.101.70]:46916 "EHLO
        foss.arm.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726063AbfEFJVC (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 6 May 2019 05:21:02 -0400
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
        by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id CA479A78;
        Mon,  6 May 2019 02:21:01 -0700 (PDT)
Received: from big-swifty.misterjones.org (usa-sjc-mx-foss1.foss.arm.com [217.140.101.70])
        by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPSA id 899323F5AF;
        Mon,  6 May 2019 02:21:00 -0700 (PDT)
Date:   Mon, 06 May 2019 10:21:10 +0100
Message-ID: <86k1f49bw9.wl-marc.zyngier@arm.com>
From:   Marc Zyngier <marc.zyngier@arm.com>
To:     Heyi Guo <guoheyi@huawei.com>
Cc:     <linux-kernel@vger.kernel.org>,
        wanghaibin 00208455 <wanghaibin.wang@huawei.com>,
        kvmarm <kvmarm@lists.cs.columbia.edu>
Subject: Re: ARM/gic-v4: deadlock occurred
In-Reply-To: <db3bb9c1-8b3a-760d-057f-b8fb6914809b@huawei.com>
References: <9efe0260-4a84-7489-ecdd-2e9561599320@huawei.com>
        <86lfzl9ofe.wl-marc.zyngier@arm.com>
        <db3bb9c1-8b3a-760d-057f-b8fb6914809b@huawei.com>
User-Agent: Wanderlust/2.15.9 (Almost Unreal) SEMI-EPG/1.14.7 (Harue)
 FLIM/1.14.9 (=?UTF-8?B?R29qxY0=?=) APEL/10.8 EasyPG/1.0.0 Emacs/26
 (aarch64-unknown-linux-gnu) MULE/6.0 (HANACHIRUSATO)
Organization: ARM Ltd
MIME-Version: 1.0 (generated by SEMI-EPG 1.14.7 - "Harue")
Content-Type: text/plain; charset=US-ASCII
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Sun, 05 May 2019 12:15:51 +0100,
Heyi Guo <guoheyi@huawei.com> wrote:
> 
> BTW since its_irq_set_vcpu_affinity() is already in atomic context,
> do we really need a separate lock its_dev->event_map.vlpi_lock? I
> didn't find anywhere outside its_irq_set_vcpu_affinity() call chain
> acquires this lock.

The reason is that the vlpi_maps array covers the whole of the
generating device, and not just a single interrupt. Relying on the
irq_desc lock to protect the array wouldn't work, as you could still
have concurrent accesses to the array (map, unmap and get all access
the same data).

So one way or another, we need some form of mutual exclusion at this
level. I guess one of the design mistakes that we have in the current
code is that there is no "device wide" operation, and that we rely on
map/unmap to perform the allocations on demand in the low level code.

What we could potentially do would be to move this allocation higher
up in the stack, and track the first time an LPI is turned into a VLPI
at that level. That's an invasive change though...

Thanks,

	M.

-- 
Jazz is not dead, it just smell funny.
